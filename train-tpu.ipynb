{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import sys\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport tensorflow as tf\nimport tensorflow.keras as keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.constraints import MaxNorm\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.models import Model, load_model\nfrom tensorflow.keras.callbacks import EarlyStopping, CSVLogger,ModelCheckpoint\n\nfrom kaggle_datasets import KaggleDatasets\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"GCS_PATH = KaggleDatasets().get_gcs_path('variablelength-handwritten-digits')\n\nPATH=GCS_PATH+'/*.tfrecords'\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"@tf.function\ndef num_char_acc(y_true,y_pred):\n    y_true = tf.convert_to_tensor(y_true)\n    _,num = tf.split(y_true, [40,4], 1)\n    _,NUM = tf.split(y_pred, [40,4], 1)\n    num_char_true = tf.math.argmax(num,axis=-1,output_type=tf.int32)\n    num_char_pred = tf.math.argmax(NUM,axis=-1,output_type=tf.int32)\n    n =tf.cast(num_char_true==num_char_pred,tf.float32)\n    return tf.reduce_mean(n)\n\n@tf.function\ndef custom_acc(y_true,y_pred):\n\n    y_true = tf.convert_to_tensor(y_true)\n    a,b,c,d ,num = tf.split(y_true, [10,10,10,10,4], 1)\n    true = [a,b,c,d]\n    A,B,C,D,_ = tf.split(y_pred, [10,10,10,10,4], 1)\n    pred = [A,B,C,D]\n    num_char_true = tf.math.argmax(num,axis=-1,output_type=tf.int32)+ 1\n    i = 0 \n    n =tf.constant(0,tf.int32)\n    while (i<4) :\n        char_pred = tf.math.argmax(pred[i],axis=-1,output_type=tf.int32)\n        char_true = tf.math.argmax(true[i],axis=-1,output_type=tf.int32)\n        n = n+ tf.reduce_sum(tf.cast((char_pred==char_true)&(tf.convert_to_tensor(i)<num_char_true),tf.int32))\n        i=i+1\n    accuracy = tf.divide(tf.cast(n,tf.float32) , tf.cast(tf.reduce_sum(num_char_true),tf.float32))\n    return accuracy\n\n@tf.function\ndef custom_loss(y_true,y_pred):\n    alpha =5\n    y_true = tf.convert_to_tensor(y_true)\n    a,b,c,d ,num = tf.split(y_true, [10,10,10,10,4], 1)\n    true = [a,b,c,d]\n    A,B,C,D,NUM = tf.split(y_pred, [10,10,10,10,4], 1)\n    pred = [A,B,C,D]\n    num_char_true = tf.math.argmax(num,axis=-1,output_type=tf.int32)+ 1\n    loss_1 = keras.losses.categorical_crossentropy(num ,NUM)\n    i =0\n    loss =tf.zeros_like(loss_1,dtype=tf.float32)\n    #loss =tf.zeros([32,1],tf.float32)\n    while (i<4) :\n        char_pred = pred[i]\n        char_true = true[i]\n        loss = loss + tf.cast(tf.convert_to_tensor(i)<num_char_true,tf.float32)*keras.losses.categorical_crossentropy(char_true,char_pred)\n        i=i+1\n    return alpha * tf.reduce_sum(loss_1) + tf.reduce_sum(loss) \n\ndef build_and_compile_model():\n    \n    rate =0.2\n    rate2 = 0.5\n    inputs = keras.Input(shape=(128, 128,1), name='img')\n    #x = layers.Conv2D(16, 5, activation='relu',kernel_constraint=MaxNorm(3),padding='same')(inputs)\n    x = layers.Conv2D(16, 5, activation='relu',padding='same')(inputs)\n    x = layers.Dropout(rate)(x)\n    x = layers.Conv2D(16, 5, activation='relu',padding='same')(x)\n    x = layers.Dropout(rate)(x)\n    x = layers.MaxPooling2D(4)(x)\n    x = layers.Conv2D(32, 5, activation='relu',padding='same')(x)\n    x = layers.Dropout(rate)(x)\n    x = layers.Conv2D(32, 5, activation='relu',padding='same')(x)\n    x = layers.Dropout(rate)(x)\n    x = layers.MaxPooling2D(4)(x)\n    x = layers.Conv2D(64, 5, activation='relu',padding='same')(x)\n    x = layers.Dropout(rate)(x)\n    x = layers.Conv2D(64, 5, activation='relu',padding='same')(x)\n    x = layers.Dropout(rate)(x)\n    x = layers.MaxPooling2D(4)(x)\n   \n    feature = layers.Flatten()(x)\n    feature = layers.Dropout(rate2)(feature)\n    outputs = layers.concatenate([Dense(10, activation='softmax')(feature),Dense(10, activation='softmax')(feature),\n                                  Dense(10, activation='softmax')(feature),Dense(10, activation='softmax')(feature),\n                                  Dense(4, activation='softmax', name='num_digits')(feature)])\n    \n    model = keras.Model(inputs=inputs , outputs=outputs, name='mnist_model')\n    model.compile(loss=custom_loss,optimizer=keras.optimizers.RMSprop(learning_rate=0.001)\n                  ,metrics=[custom_acc,num_char_acc])\n    return model\ndef parse_func(s_example):\n    features = {\n                'label':tf.io.FixedLenFeature((44,),tf.float32),\n                'image':tf.io.FixedLenFeature((128,128,1),tf.float32),\n                }\n    example = tf.io.parse_single_example(s_example, features=features)\n    return example['image'],example['label']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"PATH","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"with tpu_strategy.scope():\n    model = build_and_compile_model()\n    \n    BATCH_SIZE=256\n    dataset = tf.data.Dataset.list_files(PATH).\\\n        interleave(tf.data.TFRecordDataset, cycle_length=4, block_length=1).map(parse_func)\n\n    train_dataset = dataset.skip(6000).repeat().shuffle(4000).batch(BATCH_SIZE,drop_remainder=True)\n    val_dataset = dataset.take(6000).batch(256,drop_remainder=True)\n\ncallbacks = [#ModelCheckpoint(filepath='/kaggle/working/chkpt',save_weights_only=True),\n             #EarlyStopping(monitor='val_custom_acc', mode='max', patience=10),\n             #CSVLogger('training.log')\n            ]\nmodel.fit(train_dataset,validation_data=val_dataset, epochs=10, steps_per_epoch=24000//BATCH_SIZE, callbacks=callbacks)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.save('/kaggle/working/model.h5')\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls -al /kaggle/working","execution_count":null,"outputs":[]},{"metadata":{"trusted":false},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.9"}},"nbformat":4,"nbformat_minor":4}